{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5525e861-2852-4a76-9ec5-754334cd5e10",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9ce51-2e89-4f56-a07a-c32bc6ba03a2",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7843b9b0-7b3a-48eb-af10-6cfb2806dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lat-lon-parser in /home/akuilera/.local/lib/python3.13/site-packages (1.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /home/akuilera/.local/lib/python3.13/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# To scratch the web\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# To make Timestamps\n",
    "!pip install lat-lon-parser\n",
    "from lat_lon_parser import parse\n",
    "import datetime\n",
    "\n",
    "# To connect to MySQL\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "\n",
    "# To read the .env file\n",
    "!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# For the timezone cleaning\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d389e83-72b1-47b0-9e87-75ff02650a3a",
   "metadata": {},
   "source": [
    "## Obtain the details of the parsed city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09236818-07e2-42b4-88a0-361497a87274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCityInfo(city, verbose=False):\n",
    "    city_data = []\n",
    "    \n",
    "    # Create the URL to parse to BeautifulSoup\n",
    "    url_ = 'https://en.wikipedia.org/wiki/'\n",
    "    url_city = url_ + city\n",
    "    response_city = requests.get(url_city)\n",
    "    \n",
    "    # Create soup\n",
    "    complete_soup = BeautifulSoup(response_city.content, 'html.parser')\n",
    "    soup_city = complete_soup.find(class_='infobox ib-settlement vcard')\n",
    "    \n",
    "    # Get city and country name\n",
    "    city_name = complete_soup.find('h1', class_='firstHeading').get_text()\n",
    "    city_country = soup_city.select('td.infobox-data')[0].get_text().replace('\\xa0', '').strip()\n",
    "    \n",
    "    # Get the code of the city\n",
    "    city_code = soup_city.select('td.infobox-data nickname')#find(class_='').get_text()\n",
    "    \n",
    "    # Get latitude and longitude\n",
    "    city_latitude = soup_city.find(class_='latitude').get_text()\n",
    "    city_latitude = parse(city_latitude)\n",
    "    city_longitude = soup_city.find(class_='longitude').get_text()\n",
    "    city_longitude = parse(city_longitude)\n",
    "    \n",
    "    # Get population\n",
    "    city_population = soup_city.find(string='Population').find_next(class_='infobox-data').get_text()\n",
    "    \n",
    "    # Get Timestamp\n",
    "    today = datetime.datetime.today().strftime('%d.%m.%Y %H:%M:%S')\n",
    "    \n",
    "    # Append City Data\n",
    "    city_data.append(city_name)\n",
    "    city_data.append(city_country)\n",
    "    city_data.append(city_latitude)\n",
    "    city_data.append(city_longitude)\n",
    "    city_data.append(city_population)\n",
    "    city_data.append(today)\n",
    "\n",
    "    print(f'üåê {city} data scratched')\n",
    "    \n",
    "    return city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5be97e-d762-4092-9f61-5503e83acc9d",
   "metadata": {},
   "source": [
    "### Cities Into to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ee4b2-e4af-4f68-acc6-55e05edad3f1",
   "metadata": {},
   "source": [
    "#### Define SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355b4b81-a3e1-4d14-9fb8-6d6ab4fe970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_engine(schema=\"sql_gans\", host=\"127.0.0.1\", user=\"root\", port=3306, reset_database=False, verbose=False):\n",
    "    \n",
    "    password = os.getenv(\"MYSQL_PASSWORD\")  # Make sure this is set in your environment!\n",
    "    \n",
    "    # Create connection string\n",
    "    connection_string = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
    "    if reset_database == True:\n",
    "        connection_string = f'mysql+pymysql://{user}:{password}@{host}:{port}/'\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c05d4-208a-436a-a8b5-4bcc334a0b03",
   "metadata": {},
   "source": [
    "#### Insert Cities to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d326ff06-0258-4d8d-ace6-2777854b18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Transform to Database\n",
    "def cities_to_database(cities, schema=\"sql_gans\", host=\"127.0.0.1\", user=\"root\", port=3306, reset_database=False, verbose=False):\n",
    " \n",
    "    # Reset Database\n",
    "    if reset_database == True:\n",
    "        # Set MySQL\n",
    "        # Create connection string (without specifying Database, so that it can drop database (if existing)\n",
    "        engine = sql_engine(schema=schema, host=host, user=user, port=port, reset_database=reset_database)\n",
    "    \n",
    "        # Reset Database\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(f\"DROP DATABASE IF EXISTS {schema};\"))\n",
    "            connection.execute(text(f\"CREATE DATABASE {schema};\"))\n",
    "            print(f\"üßÆ Database {schema} dropped and recreated.\")\n",
    "\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = sql_engine(schema=schema, host=host, user=user, port=port)\n",
    "\n",
    "    # Test the connection\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(text(\"SELECT 1\"))\n",
    "            print(\"‚úîÔ∏è Connection successful:\", result.scalar())\n",
    "    except Exception as e:\n",
    "        print('‚ùå Connection failed:', e)\n",
    "\n",
    "    try:\n",
    "        # Create MySQL Tables\n",
    "        with engine.connect() as conn:\n",
    "            # Create the 'country' table\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE country (\n",
    "                    country_id INT AUTO_INCREMENT, -- Automatically generated ID for each country\n",
    "                    country_name VARCHAR(255) NOT NULL, -- Name of the country\n",
    "                    PRIMARY KEY (country_id) -- Primary key to uniquely identify each city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('üåéÔ∏è Table \"country\" created')\n",
    "    except Exception as e:\n",
    "        print('üåéÔ∏è Previous \"country\" table found. No tables created')\n",
    "    \n",
    "    try:\n",
    "        # Create MySQL Tables\n",
    "        with engine.connect() as conn:\n",
    "            # Create the 'city' table\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE city (\n",
    "                    city_id INT AUTO_INCREMENT, -- Automatically generated ID for each city\n",
    "                    city_name VARCHAR(255) NOT NULL, -- Name of the city\n",
    "                    latitude FLOAT NOT NULL, -- Latitude\n",
    "                    longitude FLOAT NOT NULL, -- Longitude\n",
    "                    country_id INT NOT NULL, -- Name of the country\n",
    "                    PRIMARY KEY (city_id), -- Primary key to uniquely identify each city\n",
    "                    FOREIGN KEY (country_id) REFERENCES country(country_id) -- Foreign key to connect each population to its city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('üóæ Table \"city\" created')\n",
    "    except Exception as e:\n",
    "        print('üóæ Previous \"city\" table found. No tables created')\n",
    "        \n",
    "    try:\n",
    "        # Create MySQL Tables\n",
    "        with engine.connect() as conn:\n",
    "            # Create the 'population' table\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE city_population (\n",
    "                    population INT NOT NULL, -- Population\n",
    "                    timestamp_population DATETIME NOT NULL, -- Timestamps\n",
    "                    city_id INT NOT NULL, -- ID of the city\n",
    "                    PRIMARY KEY (city_id, timestamp_population), -- Primary key to uniquely identify each city\n",
    "                    FOREIGN KEY (city_id) REFERENCES city(city_id) -- Foreign key to connect each population to its city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Table \"city_population\" created')\n",
    "    except Exception as e:\n",
    "        print('üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Previous \"city_population\" table found. No tables created')\n",
    "        \n",
    "    # Iterate to get the info of all the listed cities\n",
    "    cities_info = []\n",
    "    for city in cities:\n",
    "        cities_info.append(getCityInfo(city))\n",
    "\n",
    "    # Transform to a Pandas DataFrame\n",
    "    cities_info_df = pd.DataFrame(cities_info)\n",
    "    cities_info_df.columns = ['city_name', 'country_name', 'latitude', 'longitude', 'population', 'timestamp_population']\n",
    "    \n",
    "    # Transform the countries to its own Dataframe\n",
    "    country_unique = cities_info_df[\"country_name\"].unique()\n",
    "    country_df = pd.DataFrame({\"country_name\": country_unique})\n",
    "\n",
    "    # Check existing data on SQL tables for Countries\n",
    "    # Read existing details\n",
    "    previous_country_sql = pd.read_sql(\"SELECT * FROM country\", con=engine)\n",
    "    # Filter country details to only those not yet in the table\n",
    "    country_df = country_df.loc[~(\n",
    "        country_df['country_name'].isin(\n",
    "            previous_country_sql['country_name']\n",
    "        )\n",
    "    ), :]\n",
    "    \n",
    "    # Send 'country_df' to SQL\n",
    "    country_df.to_sql('country',\n",
    "                    if_exists='append',\n",
    "                    con=engine,\n",
    "                    index=False)\n",
    "    print('üåéÔ∏è Table \"country\" populated')\n",
    "\n",
    "    # Fetch countries with IDs from the DB to merge with cities\n",
    "    country_from_sql = pd.read_sql(\"SELECT * FROM country\", con=engine)#, con=connection_string)\n",
    "\n",
    "    # Merge to assign country_id to cities\n",
    "    city_unique_df = cities_info_df.merge(country_from_sql,\n",
    "                                       on = \"country_name\",\n",
    "                                       how=\"left\")\n",
    "\n",
    "    # Drop columns not needed in city table\n",
    "    city_df = city_unique_df.drop(['country_name', 'population', 'timestamp_population'], axis=1)\n",
    "\n",
    "    # Check existing data on SQL Cities tables\n",
    "    # Read existing details\n",
    "    previous_city_sql = pd.read_sql(\"SELECT * FROM city\", con=engine)\n",
    "    # Filter city details to only those not yet in the table\n",
    "    city_df = city_df.loc[~(\n",
    "        city_df['city_name'].isin(\n",
    "            previous_city_sql['city_name']\n",
    "        )\n",
    "    ), :]\n",
    "\n",
    "    # Send city_df to SQL\n",
    "    city_df.to_sql('city',\n",
    "                   if_exists='append',\n",
    "                   con=engine,\n",
    "                   index=False)\n",
    "    print('üóæ Table \"city\" populated')\n",
    "\n",
    "    # Fetch cities with IDs from DB to merge with population info\n",
    "    city_from_sql = pd.read_sql(\"SELECT * FROM city\", con=engine)#, con=connection_string)\n",
    "\n",
    "    # Prepare population info by merging city IDs\n",
    "    info_unique_df = cities_info_df.merge(city_from_sql[['city_id', 'city_name']], on=\"city_name\", how=\"left\")\n",
    "\n",
    "    # Select only columns needed for city_population table\n",
    "    city_population_df = info_unique_df[['population', 'timestamp_population', 'city_id']].copy()\n",
    "\n",
    "    # Convert timestamp_population to datetime.date (MySQL DATE format)\n",
    "    city_population_df['timestamp_population'] = pd.to_datetime(\n",
    "        city_population_df['timestamp_population'],\n",
    "        format='%d.%m.%Y %H:%M:%S'\n",
    "    )#.dt.date\n",
    "\n",
    "    # Clean population column (remove commas and convert to int)\n",
    "    city_population_df['population'] = city_population_df['population'].astype(str).str.replace(',', '').astype(int)\n",
    "    \n",
    "    # Read existing data in city_population\n",
    "    previous_city_population_sql = pd.read_sql(\"SELECT population, city_id FROM city_population\", con=engine)\n",
    "    \n",
    "    merged = city_population_df.merge(\n",
    "        previous_city_population_sql,\n",
    "        on=['population', 'city_id'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    city_population_df = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    if not city_population_df.empty:\n",
    "        city_population_df.to_sql('city_population', con=engine, if_exists='append', index=False)\n",
    "        print('üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Table \"city_population\" populated')\n",
    "    else:\n",
    "        print('üë®‚Äçüë©‚Äçüëß‚Äçüë¶ No new rows to insert in \"city_population\"')\n",
    "\n",
    "    city = pd.read_sql(\"SELECT * FROM city\", con=engine)\n",
    "    print('üóæ Table \"city\" read')\n",
    "    country = pd.read_sql(\"SELECT * FROM country\", con=engine)\n",
    "    print('üåéÔ∏è Table \"country\" read')\n",
    "    city_population = pd.read_sql(\"SELECT * FROM city_population\", con=engine)\n",
    "    print('üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Table \"city_population\" read')\n",
    "\n",
    "    return city, country, city_population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84819b3f-9cb4-426e-9035-250d6420d7e4",
   "metadata": {},
   "source": [
    "### Get weather Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0114463-a2ee-4912-bb50-28c9d909cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_weather(city, verbose=False):\n",
    "    # Open Weather Map API Key\n",
    "    key = os.getenv(\"OPEN_WEATHER_API\")\n",
    "    \n",
    "    weather = []\n",
    "    \n",
    "    # Extract location data from the cities\n",
    "    for i in range(len(city)):\n",
    "        cit = city.loc[i, 'city_name']\n",
    "        lat = city.loc[i, 'latitude']\n",
    "        lon = city.loc[i, 'longitude']\n",
    "\n",
    "        # Generate and fetch URLs\n",
    "        URL = f\"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={key}&units=metric\"\n",
    "        response = requests.get(URL)\n",
    "        print(f'‚õÖÔ∏è Weather info from {cit} fetched')\n",
    "\n",
    "        # Get the JSON responses\n",
    "        response_json = response.json()\n",
    "\n",
    "        # Extract the weather list as a dictionary\n",
    "        for item in response_json['list']:\n",
    "            item['weather'] = item['weather'][0]\n",
    "\n",
    "        # Transform JSON into a DataFrame\n",
    "        response_norm = pd.json_normalize(response_json['list'])\n",
    "\n",
    "        # Fill NaN from the Rain column\n",
    "        if 'rain.3h' in response_norm.columns:\n",
    "            response_norm['rain.3h'] = response_norm['rain.3h'].fillna(0)\n",
    "        else:\n",
    "            response_norm['rain.3h'] = 0.0\n",
    "\n",
    "        # Fill NaN from the Snow column\n",
    "        if 'snow.3h' in response_norm.columns:\n",
    "            response_norm['snow.3h'] = response_norm['snow.3h'].fillna(0)\n",
    "        else:\n",
    "            response_norm['snow.3h'] = 0.0\n",
    "\n",
    "        # Connect to the Cities IDs\n",
    "        response_norm['city_id'] = city.loc[i, 'city_id']\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        response_norm_dropped = response_norm.drop([\n",
    "            'dt',\n",
    "            'main.temp_min',\n",
    "            'main.temp_max',\n",
    "            'main.pressure',\n",
    "            'main.sea_level',\n",
    "            'main.temp_kf',\n",
    "            'weather.icon',\n",
    "            'weather.id',\n",
    "            'wind.deg',\n",
    "            'wind.gust'\n",
    "        ], axis=1)\n",
    "\n",
    "        # Rename Columns\n",
    "        response_norm_dropped = response_norm_dropped.rename(columns={\n",
    "            'pop':'precipitation_probability',\n",
    "            'dt_txt':'date_time',\n",
    "            'main.temp':'temperature',\n",
    "            'main.feels_like':'feels_like',\n",
    "            'main.grnd_level':'pressure',\n",
    "            'main.humidity':'humidity',\n",
    "            'weather.main':'weather_main',\n",
    "            'weather.description':'description',\n",
    "            'clouds.all':'clouds',\n",
    "            'wind.speed':'wind_speed',\n",
    "            'sys.pod':'part_of_day',\n",
    "            'rain.3h':'rain_3h',\n",
    "            'snow.3h':'snow_3h'\n",
    "        })\n",
    "\n",
    "        # Transform date to datetime frame\n",
    "        response_norm_dropped['date_time'] = pd.to_datetime(\n",
    "            response_norm_dropped['date_time'],\n",
    "            format='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "\n",
    "        # Get Weather Timestamp\n",
    "        today = datetime.datetime.today().strftime('%d.%m.%Y %H:%M:%S')\n",
    "        today = pd.to_datetime(today, format='%d.%m.%Y %H:%M:%S')        \n",
    "        response_norm_dropped['weather_timestamp'] = today\n",
    "\n",
    "        # Insert into the weather list\n",
    "        weather.append(response_norm_dropped)\n",
    "\n",
    "    weather_df = pd.concat(weather, ignore_index=True)\n",
    "\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ec810-fcbd-4cdd-9803-4fb797ec7e01",
   "metadata": {},
   "source": [
    "#### Weather to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ce8c67b-d508-48b8-a775-7c2575a1abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_to_sql(weather, schema=\"sql_gans\", host=\"127.0.0.1\", user=\"root\", port=3306, reset_database=False, verbose=False):\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = sql_engine(schema=schema, host=host, user=user, port=port)\n",
    "    \n",
    "    # Create MySQL Weather Table\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # Drop Weather Table if reset is set\n",
    "        if reset_database == True:\n",
    "            # Delete the 'weather' table if it exists\n",
    "            conn.execute(text(\"\"\"\n",
    "               DROP TABLE IF EXISTS weather;\n",
    "            \"\"\"))\n",
    "            print(f'üå§Ô∏è Weather database dropped and recreated.')\n",
    "\n",
    "        # Create the 'weather' table\n",
    "        try:\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE weather (\n",
    "                    visibility INT NOT NULL,\n",
    "                    precipitation_probability FLOAT NOT NULL,\n",
    "                    date_time DATETIME NOT NULL,\n",
    "                    temperature FLOAT NOT NULL, -- C\n",
    "                    feels_like FLOAT NOT NULL, -- C\n",
    "                    pressure INT NOT NULL, -- pressure at groud level hPa\n",
    "                    humidity INT NOT NULL, -- %\n",
    "                    weather_main VARCHAR(255) NOT NULL, -- Group of weather parameters\n",
    "                    description VARCHAR(255) NOT NULL, -- Weather condition within the group\n",
    "                    clouds INT NOT NULL, -- Cloudness %\n",
    "                    wind_speed FLOAT NOT NULL, -- m/seg\n",
    "                    part_of_day CHAR(1) NOT NULL, -- n: night , d: day)\n",
    "                    rain_3h FLOAT NOT NULL, -- rain volume for last 3 hours mm\n",
    "                    snow_3h FLOAT NOT NULL, -- snow volume for last 3 hours mm\n",
    "                    city_id INT NOT NULL,\n",
    "                    weather_timestamp DATETIME NOT NULL, -- Timestamps\n",
    "                    PRIMARY KEY (city_id, date_time), -- Primary key to uniquely identify each city\n",
    "                    FOREIGN KEY (city_id) REFERENCES city(city_id) -- Foreign key to connect each population to its city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('‚õÖÔ∏è Table \"weather\" created')\n",
    "        except Exception as e:\n",
    "            print('‚õÖÔ∏è Previous \"weather\" table found. No tables created')\n",
    "\n",
    "    # Fetch existing primary keys from the weather table\n",
    "    existing = pd.read_sql(\"SELECT city_id, date_time FROM weather\", con=engine)\n",
    "    existing['date_time'] = pd.to_datetime(existing['date_time'])\n",
    "    \n",
    "    # Merge to find new rows\n",
    "    weather_merged = weather.merge(existing, on=['city_id', 'date_time'], how='left', indicator=True)\n",
    "    weather_new = weather_merged[weather_merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    if not weather_new.empty:\n",
    "        weather_new.to_sql('weather', if_exists='append', con=engine, index=False)\n",
    "        print('‚õÖÔ∏è Table \"weather\" populated with new rows')\n",
    "    else:\n",
    "        print('‚õÖÔ∏è No new weather rows to insert')\n",
    "\n",
    "    # Fetch from SQL\n",
    "    weather_df = pd.read_sql(\"SELECT * FROM weather\", con=engine)\n",
    "    print('‚õÖÔ∏è Table \"weather\" read')\n",
    "\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250598b-4bec-4fb0-80b9-4a32690783b6",
   "metadata": {},
   "source": [
    "### Create Airports Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95e713f-e184-4420-aa54-d6b47b382c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airports(city_tables, verbose=False):\n",
    "    # Extract cities details\n",
    "    city_ids = city_tables['city_id']\n",
    "    latitudes = city_tables['latitude']\n",
    "    longitudes = city_tables['longitude']\n",
    "    city_names = city_tables['city_name']\n",
    "\n",
    "    # API headers\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": RapidAPI_KEY,\n",
    "        \"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    querystring = {\"withFlightInfoOnly\": \"true\"}\n",
    "\n",
    "    # DataFrame to store results\n",
    "    all_airports = []\n",
    "\n",
    "    for idn, lat, lon, city_name in zip(city_ids, latitudes, longitudes, city_names):\n",
    "        # Construct the URL with the latitude and longitude\n",
    "        url = f\"https://aerodatabox.p.rapidapi.com/airports/search/location/{lat}/{lon}/km/50/16\"\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            airports = pd.json_normalize(data.get('items', []))\n",
    "            # Add city_id\n",
    "            airports['city_id'] = idn\n",
    "            all_airports.append(airports)\n",
    "\n",
    "            if verbose == True:\n",
    "                print(f'Airports for {city_name} (city_id {idn}):\\n', airports)\n",
    "\n",
    "        else:\n",
    "            print('Error: ', response.status_code)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"API response for {cit}: {response.json()}\")\n",
    "\n",
    "    airports_df = pd.concat(all_airports, ignore_index=True)\n",
    "\n",
    "    airports_df['airport_name'] = airports_df['name']\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    column_drop = [\n",
    "        # 'icao',\n",
    "        'iata',\n",
    "        'name',\n",
    "        'shortName',\n",
    "        'municipalityName',\n",
    "        'countryCode',\n",
    "        'timeZone',\n",
    "        'location.lat',\n",
    "        'location.lon'\n",
    "        ]\n",
    "\n",
    "    # Drop columns\n",
    "    airports_df.drop(\n",
    "        columns=column_drop,\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "    if verbose==True:\n",
    "        print('Airports gotten:\\n',airports_df)\n",
    "\n",
    "    return airports_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784307e-1f9d-4c21-a213-b271ea9d7e87",
   "metadata": {},
   "source": [
    "#### Insert Airports to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4edbd8b-26cc-4fc2-bf00-ab1c75d15868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def airports_to_sql(airport, schema=\"sql_gans\", host=\"127.0.0.1\", user=\"root\", port=3306, reset_database=False, verbose=False):\n",
    "\n",
    "    # Connection setup\n",
    "    engine = sql_engine(schema=schema, host=host, user=user, port=port, )\n",
    "    \n",
    "    # Create MySQL Weather Table\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # Drop Airports Table if reset is set\n",
    "        if reset_database == True:\n",
    "            # Delete the 'airports' table if it exists\n",
    "            conn.execute(text(\"\"\"\n",
    "               DROP TABLE IF EXISTS airport;\n",
    "            \"\"\"))\n",
    "            print(f\"‚úàÔ∏è Airport database dropped and recreated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the 'airport' table\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE airport (\n",
    "                \ticao VARCHAR(4) NOT NULL,\n",
    "                    airport_name VARCHAR(50) NOT NULL,\n",
    "                \tcity_id INT NOT NULL,\n",
    "                    PRIMARY KEY (icao), -- Primary key to uniquely identify each city\n",
    "                    FOREIGN KEY (city_id) REFERENCES city(city_id) -- Foreign key to connect each population to its city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('‚úàÔ∏è Table \"airport\" created')\n",
    "        except Exception as e:\n",
    "            print('‚úàÔ∏è Previous \"airport\" table found. No tables created')\n",
    "\n",
    "    if verbose:\n",
    "        print('Airports to insert:')\n",
    "        print(airport)\n",
    "    \n",
    "    # Check existing data on SQL tables for Airports\n",
    "    # Read existing details\n",
    "    previous_airport_sql = pd.read_sql(\"SELECT * FROM airport\", con=engine)\n",
    "    # Filter airport details to only those not yet in the table\n",
    "    airport = airport.loc[~(\n",
    "        airport['icao'].isin(\n",
    "            previous_airport_sql['icao']\n",
    "        )\n",
    "    ), :]\n",
    "    \n",
    "    if verbose:\n",
    "        if not previous_airport_sql.empty:\n",
    "            print('Airport from SQL:\\n', previous_airport_sql.sample())\n",
    "        else:\n",
    "            print('Airport from SQL is empty.')\n",
    "        print(previous_airport_sql.info())\n",
    "        \n",
    "        if not airport.empty:\n",
    "            print('Airport from API cleaned:\\n', airport.sample())\n",
    "        else:\n",
    "            print('Airport from API cleaned is empty.')\n",
    "        print(airport.info())\n",
    "\n",
    "    \n",
    "    # Insert to SQL\n",
    "    airport.to_sql('airport',\n",
    "                    if_exists='append',\n",
    "                    con=engine,\n",
    "                    index=False)\n",
    "    print('‚úàÔ∏è Table \"airport\" populated')\n",
    "\n",
    "    # Fetch countries with IDs from the DB to merge with cities\n",
    "    airport_from_sql = pd.read_sql(\"SELECT * FROM airport\", con=engine)#, con=connection_string)\n",
    "    print('‚úàÔ∏è Table \"airport\" read')\n",
    "\n",
    "    if verbose == True:\n",
    "        print('Airport to SQL:\\n', airport_from_sql.sample())\n",
    "        print(airport_from_sql.info())\n",
    "\n",
    "    return airport_from_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836448f-6e54-4964-9bde-5c551e7e9c50",
   "metadata": {},
   "source": [
    "### Get Arrival and Departure Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edd1bbbc-7ac2-4715-a8b0-c4c9b2c91e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_times(airports, days=1, verbose=False):\n",
    "    all_arrival_json = []\n",
    "    all_departure_json = []\n",
    "\n",
    "    # Get times\n",
    "    half_day_periods = 2 * days\n",
    "    times_to_get = half_day_periods + 1\n",
    "    times = []\n",
    "    periods = []\n",
    "\n",
    "    # Populate times list\n",
    "    for t in range(times_to_get):\n",
    "        time_t = pd.Timestamp.now(tz='Europe/Berlin')\n",
    "        time_t = time_t + (t * datetime.timedelta(hours=12))\n",
    "        times.append(time_t)\n",
    "    # Get time periods\n",
    "    for p in range(half_day_periods):\n",
    "        time_p0 = times[p].strftime('%Y-%m-%dT%H:%M')\n",
    "        time_p1 =  times[p+1] - datetime.timedelta(minutes=1)\n",
    "        time_p1 = time_p1.strftime('%Y-%m-%dT%H:%M')\n",
    "        period_p = time_p0 + '/' + time_p1\n",
    "        periods.append(period_p)\n",
    "\n",
    "    for icao in airports['icao']:\n",
    "        for period in periods:\n",
    "    \n",
    "            url = f\"https://aerodatabox.p.rapidapi.com/flights/airports/icao/{icao}/{period}\"\n",
    "    \n",
    "            querystring = {\"withLeg\":\"false\",\"direction\":\"Both\",\"withCancelled\":\"false\",\"withCodeshared\":\"true\",\"withCargo\":\"false\",\"withPrivate\":\"true\",\"withLocation\":\"false\"}\n",
    "    \n",
    "            headers = {\n",
    "                \"X-RapidAPI-Key\": RapidAPI_KEY,\n",
    "                \"x-rapidapi-host\": \"aerodatabox.p.rapidapi.com\"\n",
    "                }\n",
    "    \n",
    "            # Get query\n",
    "            response_raw = requests.get(url, headers=headers, params=querystring)\n",
    "    \n",
    "            if response_raw.status_code == 200:\n",
    "                # Separate the arrivals from the departures\n",
    "                response_json_arrival = response_raw.json()['arrivals']\n",
    "                response_json_departure = response_raw.json()['departures']\n",
    "        \n",
    "                # Specify airport\n",
    "                [item.update({'icao': icao}) for item in response_json_arrival]\n",
    "                [item.update({'icao': icao}) for item in response_json_departure]\n",
    "    \n",
    "                # Extend the dictionary with all the values\n",
    "                all_arrival_json.extend(response_json_arrival)\n",
    "                all_departure_json.extend(response_json_departure)\n",
    "\n",
    "\n",
    "    # Normalize dictionaries\n",
    "    response_norm_arrival = pd.json_normalize(all_arrival_json)\n",
    "    response_norm_departure = pd.json_normalize(all_departure_json)\n",
    "    \n",
    "    # Columns to rename\n",
    "    rename_columns = {\n",
    "        # 'movement.airport.icao': 'icao',\n",
    "        'movement.scheduledTime.local': 'scheduledTime',\n",
    "        'movement.revisedTime.local': 'revisedTime'\n",
    "    }\n",
    "\n",
    "    # Rename columns\n",
    "    response_norm_arrival.rename(columns=rename_columns, inplace=True)\n",
    "    response_norm_departure.rename(columns=rename_columns, inplace=True)\n",
    "\n",
    "    # Get only the columns of interest\n",
    "    response_arrival = pd.DataFrame({\n",
    "        'number': response_norm_arrival['number'], \n",
    "        'icao': response_norm_arrival['icao'], \n",
    "        'scheduledTime': response_norm_arrival['scheduledTime'], \n",
    "        'revisedTime': response_norm_arrival['revisedTime']\n",
    "        })\n",
    "    response_departure = pd.DataFrame({\n",
    "        'number': response_norm_departure['number'], \n",
    "        'icao': response_norm_departure['icao'], \n",
    "        'scheduledTime': response_norm_departure['scheduledTime'], \n",
    "        'revisedTime':response_norm_departure['revisedTime']\n",
    "        })\n",
    "\n",
    "    def remove_tz_offset(s):\n",
    "        # Removes trailing timezone offset like +02:00 or -04:00\n",
    "        return re.sub(r'([+-]\\d{2}:\\d{2})$', '', str(s)).strip()\n",
    "    \n",
    "    for df in [response_arrival, response_departure]:\n",
    "        for col in ['scheduledTime', 'revisedTime']:\n",
    "            df[col] = df[col].apply(remove_tz_offset)\n",
    "    \n",
    "    # Convert times to datetime.date (MySQL DATE format) [ex. 2025-05-23 16:30]\n",
    "    response_arrival['scheduledTime'] = pd.to_datetime(\n",
    "        response_arrival['scheduledTime'],\n",
    "        format='%Y-%m-%d %H:%M'\n",
    "    )\n",
    "    response_arrival['revisedTime'] = pd.to_datetime(\n",
    "        response_arrival['revisedTime'],\n",
    "        format='%Y-%m-%d %H:%M'\n",
    "    )\n",
    "    response_departure['scheduledTime'] = pd.to_datetime(\n",
    "        response_departure['scheduledTime'],\n",
    "        format='%Y-%m-%d %H:%M'\n",
    "    )\n",
    "    response_departure['revisedTime'] = pd.to_datetime(\n",
    "        response_departure['revisedTime'],\n",
    "        format='%Y-%m-%d %H:%M'\n",
    "    )\n",
    "\n",
    "    return response_arrival, response_departure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2d6c7-8bf3-4d1a-8d54-507ebc1441ae",
   "metadata": {},
   "source": [
    "#### Arrivals and Departures into SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cd499b9-e86d-44ab-b59a-6bd20eb43dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flights_to_sql(arrivals, departures, schema=\"sql_gans\", host=\"127.0.0.1\", user=\"root\", port=3306, reset_database=False, verbose=False):\n",
    "    \n",
    "    # Connection setup\n",
    "    engine = sql_engine(schema=schema, host=host, user=user, port=port, )\n",
    "    \n",
    "    # Create MySQL Flights Tables\n",
    "    with engine.connect() as conn:\n",
    "        \n",
    "        # Drop Flights Tables if reset is set\n",
    "        if reset_database == True:\n",
    "            # Delete the 'arrival' table if it exists\n",
    "            conn.execute(text(\"\"\"\n",
    "               DROP TABLE IF EXISTS arrival;\n",
    "            \"\"\"))\n",
    "            print(f\"üõ¨ Arrival database dropped and recreated.\")\n",
    "            # Delete the 'departure' table if it exists\n",
    "            conn.execute(text(\"\"\"\n",
    "               DROP TABLE IF EXISTS departure;\n",
    "            \"\"\"))\n",
    "            print(f\"üõ´ Departure database dropped and recreated.\")\n",
    "\n",
    "        try:\n",
    "            # Create the 'arrival' table\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE arrival (\n",
    "                \tarrival_id INT AUTO_INCREMENT, -- Automatically generated ID for each \n",
    "                \tnumber VARCHAR(10) NOT NULL,\n",
    "                \ticao VARCHAR(4) NOT NULL,\n",
    "                \tscheduledTime DATETIME NOT NULL, -- Scheduled Time\n",
    "                \trevisedTime DATETIME NOT NULL, -- Revised Time\n",
    "                    PRIMARY KEY (arrival_id), -- Primary key to uniquely identify each city\n",
    "                    UNIQUE (number, scheduledTime), -- Recommended\n",
    "                    FOREIGN KEY (icao) REFERENCES airport(icao) -- Foreign key to connect each population to its city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('üõ¨ Table \"arrival\" created')\n",
    "        except Exception as e:\n",
    "            print('üõ¨ Previous \"arrival\" table found. No tables created')\n",
    "            \n",
    "        try:\n",
    "            # Create the 'departure' table\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE departure (\n",
    "                \tdeparture_id INT AUTO_INCREMENT, -- Automatically generated ID for each \n",
    "                \tnumber VARCHAR(10) NOT NULL,\n",
    "                \ticao VARCHAR(4) NOT NULL,\n",
    "                \tscheduledTime DATETIME NOT NULL, -- Scheduled Time\n",
    "                \trevisedTime DATETIME NOT NULL, -- Revised Time\n",
    "                    PRIMARY KEY (departure_id), -- Primary key to uniquely identify each city\n",
    "                    UNIQUE (number, scheduledTime), -- Recommended\n",
    "                    FOREIGN KEY (icao) REFERENCES airport(icao) -- Foreign key to connect each population to its city\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print('üõ´ Table \"departure\" created')\n",
    "        except Exception as e:\n",
    "            print('üõ´ Previous \"departure\" table found. No tables created')\n",
    "    \n",
    "    # Check existing data on SQL tables for Arrivals\n",
    "    # Read existing details\n",
    "    previous_arrival_sql = pd.read_sql(\"SELECT * FROM arrival\", con=engine)\n",
    "    previous_departure_sql = pd.read_sql(\"SELECT * FROM departure\", con=engine)\n",
    "\n",
    "    # Antes de insertar en SQL, rellena nulos en revisedTime\n",
    "    arrivals['revisedTime'] = arrivals['revisedTime'].fillna(arrivals['scheduledTime'])\n",
    "    departures['revisedTime'] = departures['revisedTime'].fillna(departures['scheduledTime'])\n",
    "\n",
    "    if verbose == True:\n",
    "        if not previous_arrival_sql.empty:\n",
    "            print('Arrival from SQL:\\n', previous_arrival_sql.sample())\n",
    "            print(previous_arrival_sql.info())\n",
    "        else:\n",
    "            print('Arrival from SQL is empty.')\n",
    "        \n",
    "        if not previous_departure_sql.empty:\n",
    "            print('Departure from SQL:\\n', previous_departure_sql.sample())\n",
    "            print(previous_departure_sql.info())\n",
    "        else:\n",
    "            print('Departure from SQL is empty.')\n",
    "        if not arrivals.empty:\n",
    "            print('Arrival from API:\\n', arrivals.sample())\n",
    "            print(arrivals.info())\n",
    "        else:\n",
    "            print('Arrival from API is empty.')\n",
    "        \n",
    "        if not departures.empty:\n",
    "            print('Departure from API:\\n', departures.sample())\n",
    "            print(departures.info())\n",
    "        else:\n",
    "            print('Departure from API is empty.')\n",
    "\n",
    "    # Justo antes del merge en flights_to_sql:\n",
    "    arrivals['scheduledTime'] = pd.to_datetime(arrivals['scheduledTime']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    previous_arrival_sql['scheduledTime'] = pd.to_datetime(previous_arrival_sql['scheduledTime']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    arrivals = arrivals.drop_duplicates(subset=['number', 'scheduledTime'])\n",
    "\n",
    "    # Justo antes del merge en flights_to_sql:\n",
    "    departures['scheduledTime'] = pd.to_datetime(departures['scheduledTime']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    previous_departure_sql['scheduledTime'] = pd.to_datetime(previous_departure_sql['scheduledTime']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    departures = departures.drop_duplicates(subset=['number', 'scheduledTime'])\n",
    "\n",
    "    merged = arrivals.merge(\n",
    "        previous_arrival_sql[['number', 'scheduledTime']],\n",
    "        on=['number', 'scheduledTime'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    arrivals = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    merged = departures.merge(\n",
    "        previous_departure_sql[['number', 'scheduledTime']],\n",
    "        on=['number', 'scheduledTime'],\n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    departures = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    \n",
    "    # Insert to SQL\n",
    "    arrivals.to_sql('arrival',\n",
    "                    if_exists='append',\n",
    "                    con=engine,\n",
    "                    index=False)\n",
    "    print('üõ¨ Table \"arrival\" populated')\n",
    "    departures.to_sql('departure',\n",
    "                    if_exists='append',\n",
    "                    con=engine,\n",
    "                    index=False)\n",
    "    print('üõ´ Table \"departure\" populated')\n",
    "\n",
    "    # Fetch arrivals\n",
    "    arrival_from_sql = pd.read_sql(\"SELECT * FROM arrival\", con=engine)#, con=connection_string)\n",
    "    print('üõ¨ Table \"arrival\" read')\n",
    "    # Fetch departures\n",
    "    departure_from_sql = pd.read_sql(\"SELECT * FROM departure\", con=engine)#, con=connection_string)\n",
    "    print('üõ´ Table \"departure\" read')\n",
    "\n",
    "    return arrival_from_sql, departure_from_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e090a2b-8268-4956-a7a2-366f673fee05",
   "metadata": {},
   "source": [
    "## Fetch all the details at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e17ebf-bfa6-4b09-84de-86e5c0450848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_info(cities, schema=\"sql_gans\", host=\"127.0.0.1\", user=\"root\", port=3306, days=1, reset_database=False, verbose=False):\n",
    "\n",
    "    # Transform cities to list if it is only one\n",
    "    if type(cities) != list:\n",
    "        cities = [cities]\n",
    "        \n",
    "    # Get basic cities info\n",
    "    city_df, country_df, city_population_df = cities_to_database(\n",
    "        cities, \n",
    "        schema=schema, \n",
    "        host=host,\n",
    "        user=user,\n",
    "        port=port,\n",
    "        reset_database=reset_database, \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Fetch cities weather\n",
    "    weather = get_city_weather(city_df, verbose=verbose)\n",
    "\n",
    "    # Turn weather pd dataframe to sql\n",
    "    weather_df = weather_to_sql(\n",
    "        weather,\n",
    "        reset_database=reset_database, \n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get airports\n",
    "    airports = get_airports(city_df)\n",
    "    airport_df = airports_to_sql(\n",
    "        airports, \n",
    "        schema=schema, \n",
    "        host=host,\n",
    "        user=user,\n",
    "        port=port,\n",
    "        reset_database=reset_database,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Get flights\n",
    "    arrivals, departures = get_times(airport_df, days=days, verbose=verbose)\n",
    "\n",
    "    arrival_df, departure_df = flights_to_sql(\n",
    "        arrivals,\n",
    "        departures,\n",
    "        schema=\"sql_gans\",\n",
    "        host=\"127.0.0.1\", \n",
    "        user=\"root\", \n",
    "        port=3306, \n",
    "        reset_database=False,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    print('‚úÖ FINISHED ‚úÖ')\n",
    "\n",
    "    return city_df, country_df, city_population_df, weather_df, airport_df, arrival_df, departure_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eaacf6-bd67-4f93-83d4-4f3d56146291",
   "metadata": {},
   "source": [
    "# Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c3046-bb50-4180-98d0-5b8d9533c1ac",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d939d248-e739-488f-bb32-470faac45bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_database = True\n",
    "verbose = False\n",
    "days = 1\n",
    "\n",
    "cities = [\n",
    "    'Berlin',\n",
    "    'Hamburg',\n",
    "    'Frankfurt',\n",
    "    'Cologne',\n",
    "    'Caracas',\n",
    "    'Lima',\n",
    "    'Freiburg',\n",
    "    'Auckland',\n",
    "    'Cusco',\n",
    "    'Santiago',\n",
    "    'Arequipa',\n",
    "    'Paris',\n",
    "    'Lyon',\n",
    "    'Buenos_Aires',\n",
    "    'Mumbay',\n",
    "    'London',\n",
    "    'Heidelberg',\n",
    "    'Munich'\n",
    "    ]\n",
    "\n",
    "schema = \"sql_gans\"\n",
    "host = \"127.0.0.1\"\n",
    "user = \"root\"\n",
    "port = 3306\n",
    "RapidAPI_KEY = os.getenv(\"RapidAPI_KEY\")  # Make sure this is set in your environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3e5c2-b2cb-4c8c-a7c5-1b70e984fc79",
   "metadata": {},
   "source": [
    "# RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c727347-9511-42fb-ba88-c9e137802407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Database sql_gans dropped and recreated.\n",
      "‚úîÔ∏è Connection successful: 1\n",
      "üåéÔ∏è Table \"country\" created\n",
      "üóæ Table \"city\" created\n",
      "üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Table \"city_population\" created\n",
      "üåê Berlin data scratched\n",
      "üåê Hamburg data scratched\n",
      "üåê Frankfurt data scratched\n",
      "üåê Cologne data scratched\n",
      "üåê Caracas data scratched\n",
      "üåê Lima data scratched\n",
      "üåê Freiburg data scratched\n",
      "üåê Auckland data scratched\n",
      "üåê Cusco data scratched\n",
      "üåê Santiago data scratched\n",
      "üåê Arequipa data scratched\n",
      "üåê Paris data scratched\n",
      "üåê Lyon data scratched\n",
      "üåê Buenos_Aires data scratched\n",
      "üåê Mumbay data scratched\n",
      "üåê London data scratched\n",
      "üåê Heidelberg data scratched\n",
      "üåê Munich data scratched\n",
      "üåéÔ∏è Table \"country\" populated\n",
      "üóæ Table \"city\" populated\n",
      "üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Table \"city_population\" populated\n",
      "üóæ Table \"city\" read\n",
      "üåéÔ∏è Table \"country\" read\n",
      "üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Table \"city_population\" read\n",
      "‚õÖÔ∏è Weather info from Berlin fetched\n",
      "‚õÖÔ∏è Weather info from Hamburg fetched\n",
      "‚õÖÔ∏è Weather info from Frankfurt fetched\n",
      "‚õÖÔ∏è Weather info from Cologne fetched\n",
      "‚õÖÔ∏è Weather info from Caracas fetched\n",
      "‚õÖÔ∏è Weather info from Lima fetched\n",
      "‚õÖÔ∏è Weather info from Freiburg im Breisgau fetched\n",
      "‚õÖÔ∏è Weather info from Auckland fetched\n",
      "‚õÖÔ∏è Weather info from Cusco fetched\n",
      "‚õÖÔ∏è Weather info from Santiago fetched\n",
      "‚õÖÔ∏è Weather info from Arequipa fetched\n",
      "‚õÖÔ∏è Weather info from Paris fetched\n",
      "‚õÖÔ∏è Weather info from Lyon fetched\n",
      "‚õÖÔ∏è Weather info from Buenos Aires fetched\n",
      "‚õÖÔ∏è Weather info from Mumbai fetched\n",
      "‚õÖÔ∏è Weather info from London fetched\n",
      "‚õÖÔ∏è Weather info from Heidelberg fetched\n",
      "‚õÖÔ∏è Weather info from Munich fetched\n",
      "üå§Ô∏è Weather database dropped and recreated.\n",
      "‚õÖÔ∏è Table \"weather\" created\n",
      "‚õÖÔ∏è Table \"weather\" populated with new rows\n",
      "‚õÖÔ∏è Table \"weather\" read\n",
      "‚úàÔ∏è Airport database dropped and recreated.\n",
      "‚úàÔ∏è Table \"airport\" created\n",
      "‚úàÔ∏è Table \"airport\" populated\n",
      "‚úàÔ∏è Table \"airport\" read\n",
      "üõ¨ Table \"arrival\" created\n",
      "üõ´ Table \"departure\" created\n",
      "üõ¨ Table \"arrival\" populated\n",
      "üõ´ Table \"departure\" populated\n",
      "üõ¨ Table \"arrival\" read\n",
      "üõ´ Table \"departure\" read\n",
      "‚úÖ FINISHED ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "city, country, city_population, weather, airport, arrival, departure = get_city_info(\n",
    "    cities, \n",
    "    schema=schema,\n",
    "    host=host,\n",
    "    user=user, \n",
    "    port=port, \n",
    "    days=days,\n",
    "    reset_database=reset_database,\n",
    "    verbose=verbose\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
